# Dense-Jump Flow Matching Policy

This repository provides a minimal, reference implementation of the Dense-Jump Flow Matching Policy. The method introduced is designed to be **simple and effective**, making it easy to implement and extendâ€”perfect for "vibe coding" tools or integrating on top of your own flow matching policy codebase.

The main purpose of this repository is to share additional experiments and ablation studies that were not included in the original paper due to length restrictions.

## Authors

Zidong Chen, Zihao Guo, Peng Wang, ThankGod Itua Egbe, Yan Lyu, Chenghao Qian.

## Citation

If you find this work useful, please cite our paper:

```bibtex
@inproceedings{chen2025densejump,
  title={Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Offline Reinforcement Learning},
  author={Chen, Zidong and Guo, Zihao and Wang, Peng and Egbe, ThankGod Itua and Lyu, Yan and Qian, Chenghao},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2025},
  note={Accepted},
  url={https://arxiv.org/abs/2509.13574}
}
```

## Datasets

This project primarily uses datasets from **Gymnasium** and **Minari**.

*   **Gymnasium**: [https://gymnasium.farama.org/](https://gymnasium.farama.org/) - A standard API for reinforcement learning environments.
*   **Minari**: [https://minari.farama.org/](https://minari.farama.org/) - A library for hosting and managing offline reinforcement learning datasets.


## Repository Structure & Usage

### 1. Training (`train_flow.py`)
This is the main entry point for training the flow matching policy. It handles data loading (supporting Minari, D4RL, and local datasets), normalization, and the training loop.

**Usage Example:**
```bash
python train_flow.py --task pendulum_expert --epochs 100 --batch_size 256
```

### 2. Inference (`inference.py`)
Used to evaluate a trained policy in its environment. It calculates success rates and returns, and can export results to a CSV file. It automatically loads task configurations from `tasks_paths.json`.

**Usage Example:**
```bash
python inference.py --task pendulum_expert --policy_path logs/flow_matching/best_model.pth --num_episodes 10
```

### 3. Model Architecture (`model.py`)
Defines the `FlowMatchingPolicy` class. It features a flexible architecture supporting both MLP and TinyUNet backbones (for 1D sequence modeling), along with sinusoidal time embeddings. This file is self-contained and easy to modify for custom architectures.

### 4. Configuration (`tasks_paths.json`)
A JSON file mapping task names to their environment IDs and dataset paths. Modify this file to add new custom tasks or point to different datasets.

### 5. Analysis Tools (`knn/`)
This directory contains scripts for analyzing the learned vector fields, primarily used for the ablation studies mentioned in the paper.
*   **`knn/knn_build_index.py`**: Builds a k-Nearest Neighbors index on the dataset actions.
*   **`knn/validate_velocity_field.py`**: Validates the velocity field generated by the flow matching policy relative to the training data.

## Installation

First, clone the repository and navigate into the project directory:

```bash
git clone https://github.com/DenseJumpFM/DenseJump_FlowMatching.git
cd DenseJump_FlowMatching
```

Then, install the required dependencies:

```bash
pip install -r requirements.txt
```

## References

If you use Gymnasium or Minari in your work, please cite them:

```bibtex
@article{towers2023gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U and Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and others},
  journal={arXiv preprint arXiv:2310.17067},
  year={2023}
}

@article{minari2024,
  title={Minari: A Python Library for Offline Reinforcement Learning Datasets},
  author={R. C. G., Carballo and others},
  journal={arXiv preprint},
  year={2024},
  url={https://minari.farama.org/}
}
```